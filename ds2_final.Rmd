---
title: "ds2_final"
author: "Ayako, Sekiya, Maisie Sun, and Daisy Yan"
date: "2023-04-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret) 
library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)
library(olsrr)
library(splines)
library(pdp)
library(mgcv)
library(earth)
library(ggcorrplot)
library(glmnet)
library(corrplot)
library(ggpubr)
library(lime)
library(vip)
library(klaR)
library(rpart.plot)


theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Merge data
```{r data_merging}
data_2101 <- 
  read_csv("./data/2101_data.csv") %>%
  janitor::clean_names() %>%
  na.omit()

data_6360 <-
  read_csv("./data/6360_data.csv") %>%
  janitor::clean_names() %>%
  na.omit()

data <- rbind(data_2101, data_6360) %>%
  unique() %>%
  mutate(gender = as.factor(gender),
         smoking = as.factor(smoking),
         race = as.factor(race),
         hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         vaccine = as.factor(vaccine),
         severity = as.factor(severity),
         study = as.factor(study))
```

# Data partition: training and testing datasets
```{r data, warning=FALSE}
set.seed(6360) 
trRows <- createDataPartition(data$recovery_time,
                              p = .80,
                              list = F)

# training data
trainData <- data[trRows, ]
trainData_matrix <- model.matrix(recovery_time~.,data)[trRows, ]
train_x <- model.matrix(recovery_time~.,data)[trRows,-1]
train_y <- data$recovery_time[trRows]

# test data
testData <- data[-trRows, ]
testData_matrix <- model.matrix(recovery_time~.,data)[-trRows,]
test_x <- model.matrix(recovery_time~.,data)[-trRows,-1]
test_y <- data$recovery_time[-trRows]

ctrl <- trainControl(method = "cv", number = 10, repeats = 5, selectionFunction = "best", savePredictions = "all")

str(data)

```

# Exploratory analysis and data visualization 

## Summary and NA's
Summary is used for outlier visualization and to see if there are any NAs in the data.
```{r outlier_visualization, warning=FALSE}
summary(trainData)

trainData %>% 
  summarise_all(~ sum(is.na(.)))
```

## Relationship between predictor and outcome
We want to see basic visualizations between predictor and outcome through a couple of different methods.

```{r relationship_visualization, warning=FALSE}
viz_point = function(name, title) {
  z = trainData %>%
    ggplot(aes(x = name, y = recovery_time, color = study)) +
    geom_point() + 
    ggtitle(paste(title, "and recovery time by study")) + 
    xlab(title) +
    ylab("Recovery Time (days)")
  z
}

age_recovery = viz_point(name = trainData$age, title = "Age")
height_recovery = viz_point(name = trainData$height, title = "Height")
weight_recovery = viz_point(name = trainData$weight, title = "Weight")
bmi_recovery = viz_point(name = trainData$bmi, title = "BMI")
sbp_recovery = viz_point(name = trainData$sbp, title = "SBP")
ldl_recovery = viz_point(name = trainData$ldl, title = "LDL")

viz_box = function(name, title) {
  z = trainData %>%
    ggplot(aes(x = name, y = recovery_time, color = study)) +
    geom_boxplot() + 
    ggtitle(paste(title, "and recovery time by study")) + 
    xlab(title) +
    ylab("Recovery Time (days)")
  z
}

gender_recovery = viz_box(name = trainData$gender, title = "Gender")
race_recovery = viz_box(name = trainData$race, title = "Race")
smoking_recovery = viz_box(name = trainData$smoking, title = "Smoking")
hypertension_recovery = viz_box(name = trainData$hypertension, title = "Hypertension")
diabetes_recovery = viz_box(name = trainData$diabetes, title = "Diabetes")
vaccine_recovery = viz_box(name = trainData$vaccine, title = "Vaccine")
severity_recovery = viz_box(name = trainData$severity, title = "Severity")

continuous_predictor_recovery = ggarrange(height_recovery, weight_recovery, bmi_recovery, age_recovery, sbp_recovery, ldl_recovery, 
          labels = c("A", "B", "C", "D", "E", "F"),
          ncol = 3, nrow = 2)
continuous_predictor_recovery


categorical_predictor_recovery = ggarrange(gender_recovery, race_recovery, smoking_recovery, hypertension_recovery, diabetes_recovery, vaccine_recovery, severity_recovery, 
          labels = c("A", "B", "C", "D", "E", "F", "G"),
          ncol = 3, nrow = 3)
categorical_predictor_recovery

ggsave("plot/continuous_predictor_recovery.png", continuous_predictor_recovery, width = 10, height = 5, bg = "white")
ggsave("plot/categorical_predictor_recovery.png", categorical_predictor_recovery, width = 15, height = 10, bg = "white")
```

# Test multicolinearity
```{r multicolinearity_assumptions, include=FALSE, warning=FALSE}
corrplot(cor(train_x[,-c(2:7, 11:12,15:18)]), method = "circle", type = "full")
```

#Part I

# Linear model training

We will now model train for linear methods. Each model has been tuned.

## Linear regression
```{r linear, warning=FALSE}
set.seed(6360)

# Using min rule
fit.linear <- train(train_x, train_y,
                method = "lm",
                trControl = ctrl)

summary(fit.linear)

# Predict with test data
pred.linear <- predict(fit.linear, newdata = testData_matrix)

# Test error (MSE)
mean((pred.linear - test_y)^2)

# Final model
fit.linear$finalModel
```

## Elastic net
```{r enet, warning=FALSE}
set.seed(6360)

# Using min rule
fit.enet <- train(train_x, train_y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(0.5, -1.5, length = 10))),
                  trControl = ctrl)

colors <- rainbow(25)
parings <- list(superpose.symbol = list(col = colors),
                    superpose.line = list(col = colors))

plot_enet <- plot(fit.enet, par.settings = parings)
plot_enet

# Find selected tuning parameter
fit.enet$bestTune

# Predict with test data
pred.enet <- predict(fit.enet, newdata = testData_matrix)

# Test error (MSE)
mean((pred.enet - test_y)^2)

# Coefficients in the final model
coef(fit.enet$finalModel, fit.enet$bestTune$lambda)
```

## Lasso
```{r lasso, warning=FALSE}
set.seed(6360)

# Using min rule
fit.lasso <- train(train_x, train_y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(3, -7, length = 20))),
                   trControl = ctrl)

plot_lasso <- plot(fit.lasso, xTrans = log)
plot_lasso

# Find selected tuning parameter
fit.lasso$bestTune 

# Predict with test data
pred.lasso <- predict(fit.lasso, newdata = testData_matrix)

# Test error (MSE)
mean((pred.lasso - test_y)^2)

# Coefficients in the final model
coef(fit.lasso$finalModel, fit.lasso$bestTune$lambda)
```

## PLS
```{r pls, warning=FALSE}
set.seed(6360)

# Using min rule
fit.pls <- train(train_x, train_y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 3:15),
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

plot_pls <- ggplot(fit.pls, highlight = TRUE)
plot_pls

# Find selected tuning parameter
fit.pls$bestTune

# Predict with test data
pred.pls <- predict(fit.pls, newdata = testData_matrix)

# Test error (MSE)
mean((pred.pls - test_y)^2)

# Coefficients in the final model
coef(fit.pls$finalModel, fit.pls$bestTune$ncomp)
```

# Non-linear models

We will now model train for non-linear methods.Each model has been tuned.

## Generalized Additive Model(GAM)
```{r gam, warning=FALSE}
set.seed(6360)

# Using min rule
fit.gam <- train(train_x, train_y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 trControl = ctrl)

# Find selected tuning parameter
fit.gam$bestTune

# Predict with test data
pred.gam <- predict(fit.gam, newdata = testData_matrix)

# Test error (MSE)
mean((pred.gam - test_y)^2)

# Coefficients in the final model
fit.gam$finalModel
```

## Multivariate Adaptive Regression Splines (MARS)
```{r mars, warning=FALSE}
set.seed(6360)

# Using min rule
fit.mars <- train(train_x, train_y,
                  method = "earth",
                  tuneGrid = expand.grid(degree = 1:3, 
                                         nprune = 1:11),
                  trControl = ctrl)

plot_mars <- ggplot(fit.mars)
plot_mars

# Find selected tuning parameter
fit.mars$bestTune

# Predict with test data
pred.mars <- predict(fit.mars, newdata = testData_matrix)

# Test error (MSE)
mean((pred.mars - test_y)^2)

# Coefficients for final model
summary(fit.mars) %>% .$coefficients
fit.mars$finalModel

```

## Figures for cross-validation for enet, lasso, pls, and mars.
```{r plots}
ggarrange(plot_enet, plot_lasso, plot_pls, plot_mars,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

## Comparing models for continuous recovery_time
We will now use resample to see which model produces the best predictive model for COVID-19 recovery time. 

```{r non_linear_models, warning=FALSE}
resample <- resamples(list(linear = fit.linear,
                         enet = fit.enet, 
                         pls = fit.pls,
                         gam = fit.gam,
                         mars = fit.mars))
summary(resample)

bwplot(resample, metric = "RMSE")
```

# Regression tree

## Using rpart
```{r rt_rpart}
set.seed(6360)
rpart.fit <- train(recovery_time ~ . , 
                   trainData, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-12,-2.5, length = 20))),
                   trControl = ctrl)

rpart_plot <- ggplot(rpart.fit, highlight = TRUE)
rpart_plot

rpart.plot(rpart.fit$finalModel)

```

## Using ctree
```{r rt_ctree}
set.seed(6360)
ctree.fit <- train(recovery_time ~ . , 
                   trainData, 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-1, -6, length = 20))),
                   trControl = ctrl)

ctree_plot <- ggplot(ctree.fit, highlight = TRUE)
ctree_plot

plot(ctree.fit$finalModel)

```

# Random Forest
```{r random_forest_gbm}
rf.grid <- expand.grid(mtry = 1:14,
                       splitrule = "variance",
                       min.node.size = 2:8)
set.seed(6360)
rf.fit <- train(recovery_time ~ . , 
                trainData, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

rf_plot <- ggplot(rf.fit, highlight = TRUE)
rf_plot
```

# Boosting
```{r boosting_ranger}
set.seed(6360)
gbm.grid <- expand.grid(n.trees = c(500, 700, 1000, 2000, 2100, 2200, 2500, 2700, 3000, 3250),
                        interaction.depth = 1:3,
                        shrinkage = c(0.005,0.01),
                        n.minobsinnode = c(1))
set.seed(6360)
gbm.fit <- train(recovery_time ~ . , 
                 trainData, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

gbm_plot <- ggplot(gbm.fit, highlight = TRUE)
gbm_plot


```

## Figures for cross-validation for enet, lasso, pls, and mars.
```{r plots}
ggarrange(rpart_plot, ctree_plot, rf_plot, gbm_plot,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

# Comparing all models
We will now compare all the models created to each other to find the best predictive model for COVID-19 recovery time.

```{r}
resample <- resamples(list(linear = fit.linear,
                         enet = fit.enet, 
                         lasso = fit.lasso,
                         pls = fit.pls,
                         gam = fit.gam,
                         mars = fit.mars,
                         rpart = rpart.fit,
                         ctree = ctree.fit,
                         rf = rf.fit,
                         boosting = gbm.fit))
summary(resample)

bwplot(resample, metric = "RMSE")
```

# Part II

# Create Data with Binary Outcome
Instead of having recovery time as continuous, we will now binarize the outcome to short and long recover time using 30 as the cut-off point.
Coefficients will only be outputted for the best model to allow for greater running speed.

```{r data_creation}
data_binary <-
  data %>%
  mutate(recovery_time = case_when(recovery_time > 30 ~ "long", TRUE ~ "short"),
         recovery_time = as.factor(recovery_time))

set.seed(6360)
trRows_binary <- createDataPartition(data_binary$recovery_time,
                              p = .80,
                              list = F)
# training data
trainData_binary <- data_binary[trRows_binary, ]
trainData_matrix_binary <- model.matrix(recovery_time~.,data_binary)[trRows_binary, ]
train_x_binary <- model.matrix(recovery_time~.,data_binary)[trRows_binary,-1]
train_y_binary <- data_binary$recovery_time[trRows_binary]

# test data
testData_binary <- data_binary[-trRows_binary, ]
testData_matrix_binary <- model.matrix(recovery_time~.,data_binary)[-trRows_binary,]
test_x_binary <- model.matrix(recovery_time~.,data_binary)[-trRows_binary,-1]
test_y_binary <- data_binary$recovery_time[-trRows_binary]

ctrl_binary <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

# Visualization
```{r}
featurePlot(x = trainData_binary[,c(1, 5:7, 10:11)], 
            y = trainData_binary$recovery_time,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "box", pch = "|", 
            auto.key = list(columns = 1))

```

# Linear Models 

We will now model train for linear methods. Each model has been tuned.

## Logistic Regression
```{r logistic_glm}
model.glm.binary <- train(train_x_binary, 
                          train_y_binary,
                          method = "glm",
                          metric = "ROC",
                          trControl = ctrl_binary)
set.seed(6360)
pred_binary <- predict(model.glm.binary, newdata = test_x_binary, 
                       type = "prob")[,1]

```

## Penalized Logistic Regression
```{r, warning=FALSE}
glmnGrid <- expand.grid(.alpha = seq(0, 5, length = 20),
                        .lambda = exp(seq(-10, -5, length = 10)))
set.seed(6360)
model.glmn.penalized <- train(x = train_x_binary,
                    y = train_y_binary,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl_binary)

model.glmn.penalized$bestTune

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(model.glmn.penalized, par.settings = myPar, xTrans = function(x) log(x))
```

## GAM for Binary Outcome
```{r}
set.seed(6360) 
model.gam.binary <- train(train_x_binary,
                   train_y_binary,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl_binary)
```

## MARS for Binary Outcome
```{r, warning=FALSE}
set.seed(6360) 
model.mars.binary <- train(train_x_binary,
                           train_y_binary,
                           method = "earth",
                           tuneGrid = expand.grid(degree = 1:4,
                                                  nprune = 2:20),
                           metric = "ROC",
                           trControl = ctrl_binary)
plot(model.mars.binary)

test.mars <- predict(model.mars.binary, newdata = testData_matrix_binary, type = "prob")[,2]
test.mars_binary <- rep("long", length(test.mars))
test.mars_binary[test.mars > 0.5] <- "short"
mean(test.mars_binary != test_y_binary)

# Coefficients for final model
summary(model.mars.binary) %>% .$coefficients
model.mars.binary$finalModel
```

## LDA 
```{r}
set.seed(6360) 
model.lda.binary <- train(train_x_binary,
                          train_y_binary,
                          method = "lda",
                          metric = "ROC",
                          trControl = ctrl_binary)
```

## QDA
```{r}
set.seed(6360) 
model.qda.binary <- train(train_x_binary,
                          train_y_binary,
                          method = "qda",
                          metric = "ROC",
                          trControl = ctrl_binary)
```

## Naive Bayes
```{r, warning=FALSE}
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1,
                      adjust = seq(.2, 3, by = .2))
set.seed(6360)
model.nb.binary <- train(train_x_binary,
                         train_y_binary,
                         method = "nb",
                         tuneGrid = nbGrid,
                         metric = "ROC",
                         trControl = ctrl_binary)
plot(model.nb.binary)
```

# Classification Trees

## Using rpart
```{r}
set.seed(6360)
rpart.fit.binary <- train(recovery_time ~ . ,
                          data_binary,
                          subset = trRows_binary,
                          method = "rpart",
                          tuneGrid = data.frame(
                            cp = exp(seq(-6,-3, len = 50))),
                          trControl = ctrl_binary,
                          metric = "ROC")
ggplot(rpart.fit.binary, highlight = TRUE)
rpart.plot::rpart.plot(rpart.fit.binary$finalModel)
```

## Using ctree
```{r}
set.seed(6360)
ctree.fit.binary <- train(recovery_time ~ . , 
                          data_binary,
                          subset = trRows_binary,
                          method = "ctree",
                          tuneGrid = data.frame(
                            mincriterion = 1-exp(seq(-2, -1, length = 10))),
                          metric = "ROC",
                          trControl = ctrl_binary)
ggplot(ctree.fit.binary, highlight = TRUE)
plot(ctree.fit.binary$finalModel)
```

# Bagging and random forests
```{r}
# Using caret
rf.grid.binary <- expand.grid(mtry = 1:8,
                              splitrule = "gini",
                              min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(6360)
rf.fit.binary <- train(recovery_time ~ . ,
                       data_binary,
                       subset = trRows_binary,
                       method = "ranger",
                       tuneGrid = rf.grid.binary,
                       metric = "ROC",
                       trControl = ctrl_binary)
ggplot(rf.fit.binary, highlight = TRUE)
```

# Boosting
```{r}
gbmA.grid.binary <- expand.grid(n.trees = c(2000,3000,4000,5000),
                                interaction.depth = 1:6,
                                shrinkage = c(0.0005,0.001,0.002),
                                n.minobsinnode = 1)
set.seed(6360)
gbmA.fit.binary <- train(recovery_time ~ . ,
                         data_binary,
                         subset = trRows_binary,
                         tuneGrid = gbmA.grid.binary,
                         trControl = ctrl_binary,
                         method = "gbm",
                         distribution = "adaboost",
                         metric = "ROC",
                         verbose = FALSE)
ggplot(gbmA.fit.binary, highlight = TRUE)
```

# Comparing all binary models
We will now compare all the models created to each other to find the best predictive model for COVID-19 recovery time which will be represented as a binarized outcome.

```{r}
resample <- resamples(list(logistic = model.glm.binary,
                           penalized_log = model.glmn.penalized,
                           lda = model.lda.binary, 
                           qda = model.qda.binary,
                           naive_bayes = model.nb.binary,
                           gam = model.gam.binary,
                           mars = model.mars.binary,
                           rpart = rpart.fit.binary,
                           ctree = ctree.fit.binary,
                           rf = rf.fit.binary,
                           boosting = gbmA.fit.binary))
summary(resample)

bwplot(resample, metric = "ROC")
```

