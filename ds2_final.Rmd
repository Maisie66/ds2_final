---
title: "ds2_final"
author: "Maisie Sun"
date: "2023-04-29"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret) 
library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)
library(olsrr)
library(splines)
library(pdp)
library(mgcv)
library(earth)
library(ggcorrplot)
library(glmnet)
library(corrplot)
library(ggpubr)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Merge data
```{r data_merging}
data_2101 <- 
  read_csv("./data/2101_data.csv") %>%
  janitor::clean_names() %>%
  na.omit()

data_6360 <-
  read_csv("./data/6360_data.csv") %>%
  janitor::clean_names() %>%
  na.omit()

data <- rbind(data_2101, data_6360) %>%
  unique() %>%
  mutate(gender = as.factor(gender),
         smoking = as.factor(smoking),
         race = as.factor(race),
         hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         vaccine = as.factor(vaccine),
         severity = as.factor(severity),
         study = as.factor(study))
```

# Data partition: training and testing datasets
```{r data, warning=FALSE}
set.seed(6360) 
trRows <- createDataPartition(data$recovery_time,
                              p = .80,
                              list = F)

# training data
trainData <- data[trRows, ]
trainData_matrix <- model.matrix(recovery_time~.,data)[trRows, ]
train_x <- model.matrix(recovery_time~.,data)[trRows,-1]
train_y <- data$recovery_time[trRows]

# test data
testData <- data[-trRows, ]
testData_matrix <- model.matrix(recovery_time~.,data)[-trRows,]
test_x <- model.matrix(recovery_time~.,data)[-trRows,-1]
test_y <- data$recovery_time[-trRows]

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, selectionFunction = "best", savePredictions = "all")

str(data)

```

# Exploratory analysis and data visualization 

## Summary and NA's
```{r outlier_visualization, warning=FALSE}
summary(trainData)

trainData %>% 
  summarise_all(~ sum(is.na(.)))
```

## Relationship between predictor and outcome
```{r relationship_visualization, warning=FALSE}
viz_point = function(name, title) {
  z = trainData %>%
    ggplot(aes(x = name, y = recovery_time, color = study)) +
    geom_density() + 
    ggtitle(paste(title, "and recovery time by study")) + 
    xlab(title) +
    ylab("Recovery Time (days)")
  z
}

age_recovery = viz_point(name = trainData$age, title = "Age")
height_recovery = viz_point(name = trainData$height, title = "Height")
weight_recovery = viz_point(name = trainData$weight, title = "Weight")
bmi_recovery = viz_point(name = trainData$bmi, title = "BMI")
sbp_recovery = viz_point(name = trainData$sbp, title = "SBP")
ldl_recovery = viz_point(name = trainData$ldl, title = "LDL")

viz_box = function(name, title) {
  z = trainData %>%
    ggplot(aes(x = name, y = recovery_time, color = study)) +
    geom_boxplot() + 
    ggtitle(paste(title, "and recovery time by study")) + 
    xlab(title) +
    ylab("Recovery Time (days)")
  z
}

gender_recovery = viz_box(name = trainData$gender, title = "Gender")
race_recovery = viz_box(name = trainData$race, title = "Race")
smoking_recovery = viz_box(name = trainData$smoking, title = "Smoking")
hypertension_recovery = viz_box(name = trainData$hypertension, title = "Hypertension")
diabetes_recovery = viz_box(name = trainData$diabetes, title = "Diabetes")
vaccine_recovery = viz_box(name = trainData$vaccine, title = "Vaccine")
severity_recovery = viz_box(name = trainData$severity, title = "Severity")

continuous_predictor_recovery = ggarrange(height_recovery, weight_recovery, bmi_recovery, age_recovery, sbp_recovery, ldl_recovery, 
          labels = c("A", "B", "C", "D", "E", "F"),
          ncol = 3, nrow = 2)
continuous_predictor_recovery


categorical_predictor_recovery = ggarrange(gender_recovery, race_recovery, smoking_recovery, hypertension_recovery, diabetes_recovery, vaccine_recovery, severity_recovery, 
          labels = c("A", "B", "C", "D", "E", "F", "G"),
          ncol = 3, nrow = 3)
categorical_predictor_recovery

ggsave("plot/continuous_predictor_recovery.png", continuous_predictor_recovery, width = 10, height = 5, bg = "white")
ggsave("plot/categorical_predictor_recovery.png", categorical_predictor_recovery, width = 15, height = 10, bg = "white")
```

# Test multicolinearity
```{r multicolinearity_assumptions, include=FALSE, warning=FALSE}
corrplot(cor(train_x[,-c(2:7, 11:12,15:18)]), method = "circle", type = "full")
```

# Linear model training

## Linear regression
```{r linear, warning=FALSE}
set.seed(6360)

# Using min rule
fit.linear <- train(train_x, train_y,
                method = "lm",
                trControl = ctrl)

summary(fit.linear)

# Predict with test data
pred.linear <- predict(fit.linear, newdata = testData_matrix)

# Test error (MSE)
mean((pred.linear - test_y)^2)

# Final model
fit.linear$finalModel
```

## Elastic net
```{r enet, warning=FALSE}
set.seed(6360)

# Using min rule
fit.enet <- train(train_x, train_y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(0.5, -1.5, length = 50))),
                  trControl = ctrl)

colors <- rainbow(25)
parings <- list(superpose.symbol = list(col = colors),
                    superpose.line = list(col = colors))

plot_enet <- plot(fit.enet, par.settings = parings)
plot_enet

# Find selected tuning parameter
fit.enet$bestTune

# Predict with test data
pred.enet <- predict(fit.enet, newdata = testData_matrix)

# Test error (MSE)
mean((pred.enet - test_y)^2)

# Coefficients in the final model
coef(fit.enet$finalModel, fit.enet$bestTune$lambda)
```

## Ridge
```{r ridge, warning=FALSE}
set.seed(6360)

# Using min rule
fit.ridge <- train(train_x, train_y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(-3, 6, length = 100))),
                   trControl = ctrl)

plot_ridge <- plot(fit.ridge, xTrans = log)
plot_ridge

# Find selected tuning parameter
fit.ridge$bestTune

# Predict with test data
pred.ridge <- predict(fit.ridge, newdata = testData_matrix)

# Test error (MSE)
mean((pred.ridge - test_y)^2)

# Coefficients in the final model
coef(fit.ridge$finalModel, fit.ridge$bestTune$lambda)
```

## Lasso
```{r lasso, warning=FALSE}
set.seed(6360)

# Using min rule
fit.lasso <- train(train_x, train_y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(3, -7, length = 100))),
                   trControl = ctrl)

plot_lasso <- plot(fit.lasso, xTrans = log)
plot_lasso

# Find selected tuning parameter
fit.lasso$bestTune 

# Predict with test data
pred.lasso <- predict(fit.lasso, newdata = testData_matrix)

# Test error (MSE)
mean((pred.lasso - test_y)^2)

# Coefficients in the final model
coef(fit.lasso$finalModel, fit.lasso$bestTune$lambda)
```

## PLS
```{r pls, warning=FALSE}
set.seed(6360)

# Using min rule
fit.pls <- train(train_x, train_y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:19),
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

plot_pls <- ggplot(fit.pls, highlight = TRUE)
plot_pls

# Find selected tuning parameter
fit.pls$bestTune

# Predict with test data
pred.pls <- predict(fit.pls, newdata = testData_matrix)

# Test error (MSE)
mean((pred.pls - test_y)^2)

# Coefficients in the final model
coef(fit.pls$finalModel, fit.pls$bestTune$ncomp)
```

## Figures for cross-validation for lasso, ridge, and enet
```{r plots}
ggarrange(plot_ridge, plot_lasso, plot_enet, plot_pls, 
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

## Comparing linear models
```{r linear_models, warning=FALSE}
resample <- resamples(list(linear = fit.linear,
                         lasso = fit.lasso, 
                         ridge = fit.ridge, 
                         enet = fit.enet, 
                         pls = fit.pls))
summary(resample)

bwplot(resample, metric = "RMSE")
```

# Non-linear models

## Generalized Additive Model(GAM)
```{r gam, warning=FALSE}
set.seed(6360)

# Using min rule
fit.gam <- train(train_x, train_y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 trControl = ctrl)

# Find selected tuning parameter
fit.gam$bestTune

# Predict with test data
pred.gam <- predict(fit.gam, newdata = testData_matrix)

# Test error (MSE)
mean((pred.gam - test_y)^2)

# Coefficients in the final model
fit.gam$finalModel
```

## Multivariate Adaptive Regression Splines (MARS)
```{r mars, warning=FALSE}
set.seed(6360)

# Using min rule
fit.mars <- train(train_x, train_y,
                  method = "earth",
                  tuneGrid = expand.grid(degree = 1:3, 
                                         nprune = 5:20),
                  trControl = ctrl)

ggplot(fit.mars)

# Find selected tuning parameter
fit.mars$bestTune

# Predict with test data
pred.mars <- predict(fit.mars, newdata = testData_matrix)

# Test error (MSE)
mean((pred.mars - test_y)^2)

# Coefficients for final model
summary(fit.mars) %>% .$coefficients
fit.mars$finalModel

# Partial dependence plot of BMI on recovery time from final model 
pdp::partial(fit.mars, pred.var = c("bmi"), grid.resolution = 10) %>% autoplot()
```

## Comparing non-linear models
```{r non_linear_models, warning=FALSE}
resample <- resamples(list(gam = fit.gam,
                         mars = fit.mars))
summary(resample)

bwplot(resample, metric = "RMSE")
```

# Comparing all models
```{r all_models, warning=FALSE}
resample <- resamples(list(linear = fit.linear,
                         lasso = fit.lasso, 
                         ridge = fit.ridge, 
                         enet = fit.enet, 
                         pls = fit.pls,
                         gam = fit.gam,
                         mars = fit.mars))
summary(resample)

bwplot(resample, metric = "RMSE")
```

